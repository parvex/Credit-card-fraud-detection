---
title: "Credit Card Fraud Detection - Zaawansowane uczenie maszynowe"
author: "Gabriel Rębacz, Michał Belniak"
date: "24/01/2021"
output:
  html_document:
    df_print: paged
    code_folding: hide
    css: styles.css
---

# Wstęp - interpretacja tematu projektu

Wybranym zadaniem jest analiza problemu klasyfikacji na zbiorze danych _Credit Card Fraud Detection_ dostępnym w serwisie Kaggle.com.

Zakres projektu obejmuje 4 podstawowe sfery:

* wnikliwą analizę zbioru danych,
* przetworzenie zbioru danych do postaci odpowiednich dla poszczególnych modeli,
* przygotowanie, uczenie i strojenie modeli wybranych spośród dostępnych w języku R,
* ocenę i porównanie modeli wraz z adekwatnymi wnioskami.


```{r init, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8)
pdf.options(encoding = 'ISOLatin2')
Sys.setlocale(category = "LC_ALL", locale = "Polish")
install.packages("pacman", repos = "http://cran.us.r-project.org")
library(pacman)
pacman::p_load(skimr, 
               dplyr, 
               ggplot2, 
               scales, 
               corrplot, 
               tidymodels, 
               tune, 
               workflows, 
               tictoc, 
               themis, 
               reshape2, 
               glmnet, 
               tensorflow, 
               keras,
               mlbench,
               caret,
               e1071,
               randomForest,
               ranger)
def_par = par(no.readonly = TRUE)
#data loading
if (!grepl('src$', getwd())) {
  setwd("src")
}
source("common.r")
load("rdata/datadf.Rdata")
```
# Analiza danych

Zbiór danych przeanalizowano pod kątem czynników które mają lub mogą mieć wpływ na sposób ich przygotowania oraz dobór parametrów modeli. 


Pierwsze 6 rekordów zbioru danych:
```{r head}
head(data_df)
```

Mamy do czynienia ze zbiorem, zawierajacym 31 atrybutów: 28 atrybutów V1-V28 zawierających pewne informacje o transakcji, atrybuty Time oraz Amount określające moment transakcji (liczba sekund od ustalonego czasu wyjściowego) oraz jej kwotę, oraz atrybut Class, określający czy transakcja jest oszustwem (wartość 1) czy nie (wartość 0). Atrybut Class jest atrybutem docelowym w naszej analizie.

Sprawdziliśmy, czy zbiór posiada wartości NaN.

```{r nulls-check}
checkNans(data_df)
```

Podstawowa analiza statystyczna zbioru danych za pomocą funkcji skim() wygląda następujaco:

```{r summarize}
skim(data_df)
```

Wynika z niej przede wszystkim fakt, że zbiór danych jest niezbalansowany pod katem liczby wykrytych oszustw w stosunku do liczby transakcji poprawnych. Niezrównoważnie bardzo dobrze uwydatnia poniższy wykres: 

```{r rozklad-klas}
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold", size=18), axis.text=element_text(size=14), axis.title=element_text(size=16, face="bold"), legend.title = element_text(size = 16), legend.text = element_text(size = 14))

ggplot(data_df, aes(x = Class, y = prop.table(stat(count)), fill = factor(Class),
                          label = sprintf("%0.3f %%", prop.table(stat(count)) * 100))) +
  geom_bar(aes(y = (..count..) / sum(..count..))) +
  labs(y = "%", x = "Klasa") +
  geom_text(stat = 'count',
              position = position_dodge(.9), 
              vjust = -0.5, 
              size = 4) +
  scale_x_discrete(labels = c("0", "1"))+
  scale_y_continuous(labels = scales::percent)+
  ggtitle("Procentowy rozkład klas") + 
  common_theme + 
  labs(fill = "Klasa")
```

Widać także, że kolumny Amount oraz Time nie są przeskalowane. Wykonaliśmy więc standaryzację Z na tych kolumnach.

```{r scale}
data_df <- data_df %>% mutate_at(c("Amount", "Time"), ~ (scale(.) %>% as.vector()))
```

Ponadto, można zauważyć, że wartości transakcji są skupione w większości na najniższych wartościach, jednakże występują również ekstremalne wartości.

Dla oryginalnego zbioru danych stworzyliśmy macierz korelacji, aby sprawdzić, czy między atrybutami są jakieś zależności.

```{r correlation-matrix-unbalanced}
#correlation matrix for unbalanced set
cor_matrix <- corMatrix(data_df, "Macierz korelacji dla niezbalansowanego zbioru")
```

Jak widać, z macierzy korelacji dla niezbalansowanego zbioru nie wynika wiele. Przyczyną tego są zaburzenia wynikające właśnie z jego niezbalansowania. Należy sprawdzić czy zbalansowanie poprawi odczyt korelacji.

# Przetwarzanie danych

W celu dalszej analizy zbioru zastosowano przekształcenia:

* Na podstawie oryginalnego zbioru zostały stworzone dwa nowe, zbalansowane zbiory:
  + z wykorzystaniem techniki podpróbkowania (undersampling),
  + z wykorzystaniem techniki nadpróbkowania (oversampling).

Podpróbkowanie wybiera wszystkie rekordy z klasą mniejszościową i dla każdego z nich losowo dobiera wartości z klasą dominującą. Nadpróbkowanie zaś generuje nowe wartości klasy o mniejszej liczności na podstawie istniejących wartości.

## Podpróbkowanie

Zbiór poddany został przekształceniu podpróbkowania dzięki czemu powstał zbalansowany zbiór złożony z wszystkich istniejących wartości klasy stanowiącej mniejszość oraz losowo wybranych rekordów o klasie większościowej.

```{r undersampling-cor-matrix}
# Perform undersampling and plot correlation matrix

undersample_recipe <- recipe(Class~., data = data_df) %>%
  themis::step_downsample(Class)

undersampled_df <-undersample_recipe %>% prep(training = data_df) %>% bake(new_data=NULL)
n_rows <- nrow(undersampled_df)

cor_matrix <- corMatrix(undersampled_df, "Macierz korelacji dla zbioru poddanemu podpróbkowaniu.")
```

Po przeprowadzeniu operacji w zbiorze znajduje się `r n_rows` wierszy. Z macierzy korelacji dla danych zbalansowanych wynikają korelacje między wartością atrybutu Class a niektórymi z pozostałych atrybutów.

Poniżej przedstawione są wykresy pudełkowe dla atrybutów o największych korelacjach (wartość absolutna większa niż 0.5) w zbiorze poddanym przekształceniu *undersampling*.

```{r undersampling-cor-boxplot}
# Take variables which have significant correlation with Class. Use oversampled set.
class_corrs <- cor_matrix["Class", ][-ncol(undersampled_df)]
big_corrs <- names(class_corrs)[abs(class_corrs) > 0.5]

melted <- undersampled_df %>% select(all_of(big_corrs), Class) %>% melt(id.var = "Class")
ggplot(melted, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=Class)) + 
  ggtitle("Wykresy pudełkowe wartości wybranych atrybutów") +
  facet_wrap( ~ variable, scales="free") +
  labs(y = "Wartość", x = "Atrybut", fill = "Klasa") +
  common_theme

undersampled_df$Class <- as.factor(undersampled_df$Class)
```

## Nadprobkowanie

Zbiór poddany został przekształceniu nadpróbkowania z zastosowaniem techniki SMOTE, dzięki czemu powstał zbalansowany zbiór złożony z istniejących rekordów klasy mniejszościowej, nowo wygenerowanych rekordów klasy mniejszościowej (na podstawie istniejących już rekordów) oraz losowo wybranych rekordów o klasie większościowej.

```{r oversampling-cor-matrix}
# Perform oversampling and plot correlation matrix
oversample_recipe <- recipe(Class~., data = data_df) %>%
  themis::step_smote(Class)

oversampled_df <- oversample_recipe %>% prep(training = data_df) %>% bake(new_data=NULL)
n_rows <- nrow(oversampled_df)

cor_matrix <- corMatrix(oversampled_df, "Macierz korelacji dla zbioru poddanemu nadpróbkowaniu.")
```

Po przeprowadzeniu operacji w zbiorze znajduje się `r n_rows` wierszy. Z macierzy korelacji dla tego zbioru również wynikają zależności między atrybutem Class oraz niektórymi z pozostałych.

Poniżej przedstawione są wykresy pudełkowe dla atrybutów o największych korelacjach w zbiorze poddanym operacji *oversampling*.

```{r oversampling-cor-boxplot}
# Take variables which have significant correlation with Class. Use oversampled set.
class_corrs <- cor_matrix["Class", ][-ncol(oversampled_df)]
big_corrs <- names(class_corrs)[abs(class_corrs) > 0.5]

melted <- oversampled_df %>% select(all_of(big_corrs), Class) %>% melt(id.var = "Class")
ggplot(melted, aes(x=variable, y=value)) + 
  geom_boxplot(aes(fill=Class)) + 
  facet_wrap( ~ variable, scales="free") +
  ggtitle("Wykresy pudełkowe wartości wybranych atrybutów") +
  labs(y = "Wartość", x = "Atrybut", fill = "Klasa") +
  common_theme

oversampled_df$Class <- as.factor(oversampled_df$Class)
```

Zarówno w przypadku operacji *undersampling* jak i *oversampling* na przedstawionych wykresach pudełkowych widać wyraźnie różnicę w charakterystyce niektórych atrybutów w zależności od klasy. W przypadku klasy 1, czyli oszustwa, wartości mają inną średnią oraz ich kwantyle rzędu 0.25 oraz 0.75 znacząco się różnią od tych dla klasy 0. 

## Selekcja atrybutów

W tym podrozdziale przeprowadzono selekcję atrybutów za pomocą automatycznej metody wybierajacej atrybuty na podstawie wartości *variable importance* z lasu losowego. Wybierane są wartości większe od średniej wszystkich wartości. 
Wykorzystano las losowy z pakietu "ranger" złożony z 50 drzew.

```{r feature selection, include = FALSE}
load("rdata/rf_fit.Rdata")
imp <- varImp(rf_fit, scale = FALSE)
plot(imp)
imp <- data.frame(imp[1])
rownames(imp)[imp$Overall > mean(imp$Overall)]
features <- rownames(imp)[imp$Overall > mean(imp$Overall)]
```

Wybrane atrybuty: `r features`

Ponownie przeprowadzono przekształcenia podpróbkowania oraz nadpróbkowania, tym razem na danych zawierających wyłącznie wybrane atrybuty.

Najpierw wykonano przekształcenie *undersampling*:

```{r feature selection continuation - undersample dataset with selected attributes}
data_df_selected <- data_df[,which(names(data_df) %in% c(features, "Class"))]

selected_undersample_recipe <- recipe(Class~., data = data_df_selected) %>%
  themis::step_downsample(Class) %>%
  prep()

selected_undersampled_df <- bake(selected_undersample_recipe, new_data=NULL)

cor_matrix <- corMatrix(selected_undersampled_df, "Macierz korelacji dla zbioru poddanemu selekcji atrybutów oraz podpróbkowaniu.")
```

Macierz korelacji dla tak stworzonego zbioru pokazuje, że wybrane atrybuty są bardzo mocno skorelowane z pojeciem docelowym.

Następnie, wykonano przekształcenie *oversampling*.
```{r feature selection continuation - oversample dataset with selected attributes}
selected_oversample_recipe <- recipe(Class~., data = data_df_selected) %>%
  themis::step_smote(Class) %>%
  prep()

selected_oversampled_df <- bake(selected_oversample_recipe, new_data=NULL)
cor_matrix <- corMatrix(selected_oversampled_df, "Macierz korelacji dla zbioru poddanemu selekcji atrybutów oraz nadpróbkowaniu.")
```

Z macierzy korelacji wynikają silne korelacje między wybranymi atrybutami oraz pojęciem docelowym.

# Przygotowanie, uczenie i strojenie wybranych modeli

W tej sekcji przedstawiono wyniki uczenia modeli. Wybranymi przez nas typami modeli są regresja logistyczna, drzewo klasyfikacji oraz sieć neuronowa, a dokładnie perceptron dwuwartswowy.

Najpierw przeprowadzono eksperymenty na zbiorach z zastosowaniem techniki podpróbkowania, a pózniej techniki nadpróbkowania 
W ramach porównania eksperymenty zostały przeprowadzone na zbiorze z wszystkimi atrybutami oraz na zbiorze zawierajacym wyłącznie wybrane atrybuty.

Przed procesem uczenia zostały stworzone specyfikacje modeli, z których model regresji logistycznej oraz drzewa klasyfikacji poddano strojeniu hiperparametrów metodą optymalizacji Bayesowskiej. Strojonym hiperparametrem w modelu regresji logistycznej był współczynnik kary dla regularyzacji L2. W przypadku drzewa klasyfikacji strojonymi hiperparametrami były współczynnik kary za złożoność drzewa, maksymalna głębokość drzewa oraz minimalna liczba danych w węźle, przy której należy dokonać podziału.
Zbiór danych podzielono na dwa zbiory: trenujący oraz testujący.
Strojenie parametrów odbyło się na zbiorze trenującym z wykorzystaniem na nim 10-krotnej walidacji krzyżowej, w którym kryterium oceny jakości było pole pod krzywą ROC.

Modele o najlepszych parametrach zostały ostatecznie wyuczone na całym zbiorze trenującym, a ich jakość wyznaczona na zbiorze testującym.

Głównym kryterium oceny jakości modeli jest pole pod krzywą ROC. Ponadto, zmierzono także dokładność modelu (accuracy) oraz f-score. Do tworzenia modeli, uczenia oraz ich strojenia wykorzystana została biblioteka tidymodels.

## Regresja logistyczna

Wykorzystano model regresji logistycznej z metodą regularyzacji L2. Poniżej przedstawiono zestawienie wartości miar jakości modelu dla regresji logistycznej:

```{r logistic regression}
load("rdata/log_req_undersample_final_fit.Rdata")
metrics <- log_req_undersample_final_fit %>% collect_metrics()
estimates = metrics[,".estimate"]
log_reg_results <- data.frame(c("Undersampling", estimates[1,], estimates[2,], estimates[3,]))
colnames(log_reg_results) <- c("Zbiór", "Dokładność", "f-score", "Pole pod krzywą ROC")

load( "rdata/log_req_undersample_selected_final_fit.Rdata")
metrics <- log_req_undersample_selected_final_fit %>% collect_metrics()
log_reg_results <- add_metrics_to_results(metrics, log_reg_results, "Undersampling z selekcją atrybutów")

load("rdata/log_req_oversample_final_fit.Rdata")
metrics <- log_req_oversample_final_fit %>% collect_metrics()
log_reg_results <- add_metrics_to_results(metrics, log_reg_results, "Oversampling")

load("rdata/log_req_oversample_selected_final_fit.Rdata")
metrics <- log_req_oversample_selected_final_fit %>% collect_metrics()
log_reg_results <- add_metrics_to_results(metrics, log_reg_results, "Oversampling z selekcją atrybutów")

log_reg_results
```

Patrząc na najważniejsze dla nas kryterium, to znaczy na pole pod krzywą ROC, można zauważyć, że model lepiej radzi sobie w przypadku, gdy został wytrenowany na danych zawierających wszystkie atrybuty. Może to oznaczać, że zostało nie zostało wziętych pod uwagę wystarczająco dużo atrybutów przy operacji selekcji atrybutów. Nie widać natomiast wyraźnej różnicy między wynikami dla zbiorów poddanych przekształceniom podpróbkowania oraz nadpróbkowania

## Drzewo klasyfikacji

Poniżej przedstawiono zestawienie wartości miar jakości modelu dla drzewa klasyfikacji:

```{r decision tree}
load("rdata/tree_undersample_final_fit.Rdata")
metrics <- tree_undersample_final_fit %>% collect_metrics()
estimates = metrics[,".estimate"]
tree_results <- data.frame(c("Undersampling", estimates[1,], estimates[2,], estimates[3,]))
colnames(tree_results) <- c("Zbiór", "Dokładność", "f-score", "Pole pod krzywą ROC")

load( "rdata/tree_undersample_selected_final_fit.Rdata")
metrics <- tree_undersample_selected_final_fit %>% collect_metrics()
tree_results <- add_metrics_to_results(metrics, tree_results, "Undersampling z selekcją atrybutów")

load("rdata/tree_oversample_final_fit.Rdata")
metrics <- tree_oversample_final_fit %>% collect_metrics()
tree_results <- add_metrics_to_results(metrics, tree_results, "Oversampling")

load("rdata/tree_oversample_selected_final_fit.Rdata")
metrics <- tree_oversample_selected_final_fit %>% collect_metrics() 
tree_results <- add_metrics_to_results(metrics, tree_results, "Oversampling z selekcją atrybutów")

tree_results
```

W przypadku drzewa wyraźnie widać poprawę działania w przypadku zbiorów poddanych *oversampling* w stosunku do modeli, które przetwarzały zbiory poddane działaniu *undersampling*. Wskazują na to dokładność oraz f-score. Jednakże, różnice zanikają w przypadku pola pod krzywą ROC. Oznacza to, że model odpowiadający zbiorowi z nadpróbkowaniem wykrywa oszustwa skuteczniej, lecz tylko dla określonego progu użytego do obliczenia dokładności i *f-score* (0.5). Dla innych progów odcięcia jakość modeli w obydwu przypadkach może być bardziej zbliżona. Warto także zaznaczyć, że dla każdego rodzaju zbioru, wartości zarówno pola pod krzywą ROC jak i pozostałych miar są niższe niż dla modelu regresji logistycznej, co czyni regresję logistyczną względnie lepszym modelem.

## Sieć neuronowa

Wykorzystano perceptron dwuwarstwowy z 64 neuronami w warstwie ukrytej oraz optymalizator Adam. Funkcją aktywacji dla neuronów w warstwie ukrytej była funkcja ReLU. Poniżej przedstawiono wartości miar jakości modelu:

```{r neural network}
load("rdata/neural_net_undersample_final_fit.Rdata")
metrics <- neural_net_undersample_final_fit %>% collect_metrics()
estimates = metrics[,".estimate"]
neural_net_results <- data.frame(c("Undersampling", estimates[1,], estimates[2,], estimates[3,]))
colnames(neural_net_results) <- c("Zbiór", "Dokładność", "f-score", "Pole pod krzywą ROC")

load( "rdata/neural_net_undersample_selected_final_fit.Rdata")
metrics <- neural_net_undersample_selected_final_fit %>% collect_metrics()
neural_net_results <- add_metrics_to_results(metrics, neural_net_results, "Undersampling z selekcją atrybutów")

load("rdata/neural_net_oversample_final_fit.Rdata")
metrics <- neural_net_oversample_final_fit %>% collect_metrics()
neural_net_results <- add_metrics_to_results(metrics, neural_net_results, "Oversampling")

load("rdata/neural_net_oversample_selected_final_fit.Rdata")
metrics <- neural_net_oversample_selected_final_fit %>% collect_metrics() 
neural_net_results <- add_metrics_to_results(metrics, neural_net_results, "Oversampling z selekcją atrybutów")

neural_net_results
```

W przypadku zbioru poddanemu nadpróbkowaniu uzyskano najlepsze spośród wszystkich modeli dokładność oraz f-score. Są one niezwykle bliskie wartości 1, zarówno w przypadku zbioru z selekcją atrybutów jak i bez. Natomiast, pole pod krzywą ROC ma najniższe do tej pory wartości w przypadku tych zbiorów, szczególnie w przypadku zbioru z wszystkimi atrybutami, co jest zaskakujące, gdyż w przypadku regresji logistycznej uzyskano odwrotny efekt. Lepiej pod tym względem wypadają zbiory poddane operacji *undersampling*. 
Jeśli chodzi o porównanie z pozostałymi modelami, sieć neuronowa radzi sobie z wykrywaniem oszustw najlepiej spośród wszystkich modeli, o ile wytrenowano je na zbiorze poddanym nadpróbkowaniu oraz próg odcięcia wyniesie 0.5. Natomiast, krzywa ROC jest najmniej korzystna spośród wszystkich modeli.
  
# Podsumowanie

Główne założenia naszego projektu zostały zrealizowane. Dane zostały przeanalizowane pod kątem cech mogących mieć wpływ na działanie i jakość modeli. Zastosowano dwie techniki równoważenia zbiorów, co wraz z uwzględnieniem selekcji atrybutów pozwoliło przebadać zachowanie modeli na czterech różnych zbiorach treningowych.

Przeprowadzono eksperymenty na trzech różnych typach modeli klasyfikacji z uwzględnieniem strojenia ich hiperparametrów. Dokonano analizy trzech miar jakości modeli, przy czym skupiono się na polu pod krzywą ROC. Pod tym względem najlepszym modelem okazała się regresja logistyczna z wykorzystaniem podpróbkowania do tworzenia zbioru treningowego. Najsłabszym modelem pod tym wzgledem okazała się sieć neuronowa wykorzystująca jako zbiór treningowy zbiór poddany nadpróbkowanium. Jednakże, ta konfiguracja okazała się mieć najlepszą dokładność oraz *f-score*. 
